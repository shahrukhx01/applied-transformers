{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84984f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "from torch import FloatTensor, LongTensor\n",
    "\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce983890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller draft model\n",
    "draft_model = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\n",
    "draft_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# larger target model\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", return_dict_in_generate=True)\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31de7c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> FloatTensor:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return softmax(outputs.logits.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53a8c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reference: https://discuss.huggingface.co/t/generation-probabilities-how-to-compute-probabilities-of-output-scores-for-gpt2/3175\n",
    "def generate_next_token(prompt: str, model: AutoModelForCausalLM, tokenizer: AutoTokenizer) -> Tuple[LongTensor, FloatTensor]:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    generated_outputs = model.generate(**inputs, do_sample=True, num_return_sequences=1, output_scores=True, \n",
    "                                            max_new_tokens=1)\n",
    "    probs = torch.stack(generated_outputs.scores, dim=1).softmax(-1)\n",
    "    return generated_outputs, probs.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f079dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Today is a nice day\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a78d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_fn(x):\n",
    "    x_max = np.where(x > 0, x, 0)\n",
    "    return x_max / np.sum(x_max)\n",
    "\n",
    "def speculative_sampling(prompt, draft_model, draft_tokenizer, target_model, target_tokenizer, N, K):\n",
    "    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so\n",
    "    # we have to add an extra -1 term when indexing using n, T, or t\n",
    "    n = len(draft_tokenizer(prompt, return_tensors=\"pt\").input_ids[0])\n",
    "    T = n + N\n",
    "    probabilities = forward_pass(prompt, model=draft_model, tokenizer=draft_tokenizer).cpu().tolist()\n",
    "    x_draft = draft_tokenizer(prompt).input_ids\n",
    "    generation = target_tokenizer(prompt).input_ids\n",
    "    num_llm_passes = 0\n",
    "    while n < T:\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        for _ in range(K):\n",
    "            generated_outputs, probs = generate_next_token(prompt=prompt, model=draft_model, tokenizer=draft_tokenizer)\n",
    "            prompt = draft_tokenizer.decode(generated_outputs.sequences[-1])\n",
    "            x_draft.append(generated_outputs.sequences[0][-1])\n",
    "            probabilities.append(probs)\n",
    "        p = np.stack(probabilities)\n",
    "        \n",
    "#         # Step 2: target model forward passes on x_draft\n",
    "        q = forward_pass(prompt, target_model, target_tokenizer).detach().cpu().numpy()\n",
    "        num_llm_passes += 1\n",
    "\n",
    "#         # Step 3: append draft tokens based on rejection criterion and resample\n",
    "#         # a token on rejection\n",
    "        all_accepted = True\n",
    "        for _ in range(K):\n",
    "            i = n - 1\n",
    "            j = x_draft[i + 1]\n",
    "            if np.random.random() < min(1, q[i][j] / p[i][j]) and n < T:  # accepted\n",
    "                generation.append(j)\n",
    "                n += 1\n",
    "            else:  # rejected\n",
    "                if n < T:\n",
    "                    generation.append(np.argmax(max_fn(q[i] - p[i])))\n",
    "                    n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "\n",
    "#         # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            prompt = target_tokenizer.decode(generation)\n",
    "        # just keeping my sanity\n",
    "        assert n == len(generation), f\"{n} {len(generation)}\"\n",
    "\n",
    "    return target_tokenizer.decode(generation), num_llm_passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "972315bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt:\n",
      "\n",
      "Today is a nice day\n",
      "\n",
      "Generation following the prompt of new 15 tokens with 2 LLM forward passes: \n",
      "\n",
      "`Today is a nice day, but gives hope of better days to those of us who have been there`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 15 # new_tokens\n",
    "K = 15 # new_tokens_per_draft_generation\n",
    "generation, num_llm_passes = speculative_sampling(prompt, draft_model, draft_tokenizer, target_model, target_tokenizer, N=N, K=K)\n",
    "\n",
    "print(f\"\"\"\n",
    "Prompt:\n",
    "\n",
    "{prompt}\n",
    "\n",
    "Generation following the prompt of new {N} tokens with {num_llm_passes} LLM forward passes: \n",
    "\n",
    "`{generation}`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e57466",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "1. Accelerating Large Language Model Decoding with Speculative Sampling, https://arxiv.org/abs/2302.01318\n",
    "2. https://jaykmody.com/blog/speculative-sampling/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
