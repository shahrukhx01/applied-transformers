{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08468c5",
   "metadata": {},
   "source": [
    "# Goal: \n",
    "Implement the Chunked Cross-Attention proposed by `Improving language models by retrieving from trillions of tokens, Sebastian Borgeaud et. al`\n",
    "# Idea:\n",
    "\n",
    "Given an input to the decoder in the encoder-decoder setting, chunk the input into `l` chunks of length `m`. Then for each chunk retrieve `top_k` nearest neighbors from a vector database. Then perform cross attention between each chunk and its k nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6289f6",
   "metadata": {},
   "source": [
    "# Chunked Cross-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cd6f21",
   "metadata": {},
   "source": [
    "In practice you'd shift decoder's input by `m-1` tokens, hence now the input prior to attention would begin from last token of the first chunk and `m-1` tokens from the second chunk and so forth. See image below:\n",
    "\n",
    "<img src='https://miro.medium.com/v2/resize:fit:834/format:webp/1*PW1kX80dwX6mjbZZq4_QGQ.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bbb8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is the port of the Jax implementation of chunked cross-attention \n",
    "present in the appendix of the original paper to pytorch.\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9e8d90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 128 # Sequence length\n",
    "m = 16 # Chunk length\n",
    "r = 32 # Retrieval length\n",
    "k = 4 # Number of neighbours\n",
    "d = 16 # Embedding size\n",
    "l = n // m # Number of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8209cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "Q = nn.Parameter(torch.zeros(d, d))\n",
    "K = nn.Parameter(torch.zeros(d, d))\n",
    "V = nn.Parameter(torch.zeros(d, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db878c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_positional_encodings(attending_length, attended_length):\n",
    "# Classical relative positional encodings\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6beb8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_attention(chunk, neighbour):\n",
    "    m, d = chunk.shape\n",
    "    r, d = neighbour.shape\n",
    "    queries = chunk @ Q\n",
    "    keys = neighbour @ K\n",
    "    logits = queries @ keys.T\n",
    "    values = neighbour @ V\n",
    "    return logits, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9c23c402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_neighbour_cross_attention(chunk, neighbours):\n",
    "    m, d = chunk.shape\n",
    "    k, r, d = neighbours.shape\n",
    "    \n",
    "    attended_chunk = [cross_attention(chunk, neighbour) for neighbour in neighbours]\n",
    "    \"\"\"\n",
    "    extract logits, and values from each tuple of logits, \n",
    "    values with resulting respective shape: (k, m, r)\n",
    "    \"\"\"\n",
    "    logits = torch.stack([attended_item[0] for attended_item in attended_chunk]) \n",
    "    values = torch.stack([attended_item[1] for attended_item in attended_chunk])\n",
    "    assert logits.shape == (k, m, r)\n",
    "    assert values.shape == (k, r, d)\n",
    "    logits = logits.reshape((m, r * k))\n",
    "    values = values.reshape((r * k, d))\n",
    "    return nn.functional.softmax(logits) @ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be0cce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_chunk_cross_attention(observation, neighbours):\n",
    "    # shift inputs so that you attend to last token of ith chunk and\n",
    "    # m-1 tokens of (i+1)th chunk\n",
    "    observation[m-1:] = 0\n",
    "    attending_chunks = observation.reshape(l, m, d)\n",
    "    chunked_output = torch.stack([attending_chunk for attending_chunk in attending_chunks]) \n",
    "    assert chunked_output.shape == (l, m, d)\n",
    "    output = chunked_output.reshape(n, d)[:n]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1fffdf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = torch.zeros((n, d)) # Input\n",
    "neighbours = torch.zeros((l, k, r, d))\n",
    "h = multi_chunk_cross_attention(observation, neighbours)\n",
    "assert h.shape == (n, d) # Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
