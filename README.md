# Applied Transformers (PyTorch)
A playground-like experimental project to explore various transformer architectures from scratch.



## Resources:
### Intuitions:
1. Intuition behind Attention Mechanism | [Notebook](https://github.com/shahrukhx01/applied-transformers/blob/main/intuitions/0.%20Transformers%20%3E%20Understanding%20Self-Attention%20and%20Cross-Attention.ipynb)
2. Intuition behind individual Transformer Blocks | [Notebook](https://github.com/shahrukhx01/applied-transformers/blob/main/intuitions/1.%20Transformers%20%3E%20Transformer%20from%20scratch%20(Annotated%20Transformer).ipynb)

### Implementations from Scratch:
Create virtual environment:
```bash
conda create -n applied-transformers python=3.10
conda activate applied-transformers
```

Install Dependencies:
```bash
pip install -r requirements.tx
```

1 Transformer Model from Scratch {Vaswani et. al, 2017} | [Python Code](https://github.com/shahrukhx01/applied-transformers/tree/main/transformer_architectures/vanilla)
2. BERT Model from Scratch {Devlin et. al, 2018} | [Coming Soon]()
3. BART Model from Scratch {Lewis et. al, 2019} | [Coming Soon]()

## References

1. [http://nlp.seas.harvard.edu/annotated-transformer/](http://nlp.seas.harvard.edu/annotated-transformer/)
2. [https://nn.labml.ai/transformers/models.html](https://nn.labml.ai/transformers/models.html)
3. [Transformers from scratch | CodeEmporium](https://www.youtube.com/playlist?list=PLTl9hO2Oobd97qfWC40gOSU8C0iu0m2l4)
